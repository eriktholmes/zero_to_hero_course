{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033150fd-1d21-49ee-8cd7-c7457c542d7c",
   "metadata": {},
   "source": [
    "# GPT: trained on Robert Frost poems \n",
    "Following Kaparthy's [GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=7) video. Except I like Robert Frost and got in the habit of memorizing his poems to recite to my daughter when she was going to sleep.. so we will train the model on his book of poems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ad11df32-fd63-46dc-8708-6562c7b58079",
   "metadata": {},
   "outputs": [],
   "source": [
    "frosty = open('RobertFrost_Poems.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0bd7ef65-cd31-4eba-a6b1-f0f307fe907a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 538800\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the dataset: {len(frosty)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fcbb334c-51b6-43c9-b62d-306da5885932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE PASTURE \n",
      "\n",
      "\n",
      "I'm going out to clean the pasture spring\n",
      "I'll only stop to rake the leaves away\n",
      "And wait to watch the water clear, I may\n",
      "I shan't be gone long\n",
      "You come too\n",
      "I'm going out to fetch the little calf\n",
      "That's standing by the mother\n",
      "It's so young\n",
      "It totters when she licks it with her tongue\n",
      "I shan't be gone long\n",
      "You come too\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(frosty[:335])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "2ba70de8-371f-42b3-87b5-1ef6d0b6b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(frosty)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8116b103-aa25-46d3-be55-feec941f4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text to numbers\n",
    "stoi = {char: i for i, char in enumerate(chars)}\n",
    "itos = {i: char for i, char in enumerate(chars)}\n",
    "encode = lambda str: [stoi[char] for char in str]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02272a45-f0e9-41bb-a03d-400b5c28340a",
   "metadata": {},
   "source": [
    "#### Let's test the encoder/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a8d2eee6-9ccf-456e-b3bc-34c8677acd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 62, 77, 5, 76, 1, 64, 62, 77, 1, 33, 75, 72, 76, 77, 82]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('Let\\'s get Frosty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "180f163c-2418-4900-8516-2cd2b4a62727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's get Frosty\""
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([39, 62, 77, 5, 76, 1, 64, 62, 77, 1, 33, 75, 72, 76, 77, 82])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c6253-814f-431f-a541-f0cefc46d6b9",
   "metadata": {},
   "source": [
    "#### Now, to torchify the data\n",
    "1)  we create a tensor ```data``` of the encoded 'frosty' file. This is a sequence of integers that corresponds to the text via our embedding.\n",
    "2)  Partition the data into training and test split (90% will be training data, 10% will be reserved to evaluate our model as it trains)\n",
    "3)  Set block size (context window) that the model sees when making its predictions\n",
    "4)  Set batch size (number of blocks that get pulled from the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3334e1b1-3057-4843-b96e-0f238d60adb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([538800])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(frosty))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3f07dd72-ffba-48a0-af79-f97264cf4095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47, 35, 32,  1, 43, 28, 46, 47, 48, 45, 32,  1,  0,  0,  0, 36,  5, 70,\n",
       "         1, 64, 72, 66, 71, 64,  1, 72, 78, 77,  1, 77, 72,  1, 60, 69, 62, 58,\n",
       "        71,  1, 77, 65, 62,  1, 73, 58, 76, 77, 78, 75, 62,  1, 76, 73, 75, 66,\n",
       "        71, 64,  0, 36,  5, 69, 69,  1, 72, 71, 69, 82,  1, 76, 77, 72, 73,  1,\n",
       "        77, 72,  1, 75, 58, 68, 62,  1, 77, 65, 62,  1, 69, 62, 58, 79, 62, 76,\n",
       "         1, 58, 80, 58, 82,  0, 28, 71, 61,  1, 80, 58, 66, 77,  1, 77, 72,  1,\n",
       "        80, 58, 77, 60, 65,  1, 77, 65, 62,  1, 80, 58, 77, 62, 75,  1, 60, 69,\n",
       "        62, 58, 75,  9,  1, 36,  1, 70, 58, 82,  0, 36,  1, 76, 65, 58, 71,  5,\n",
       "        77,  1, 59, 62,  1, 64, 72, 71, 62,  1, 69, 72, 71, 64,  0, 52, 72, 78,\n",
       "         1, 60, 72, 70, 62,  1, 77, 72, 72,  0, 36,  5, 70,  1, 64, 72, 66, 71,\n",
       "        64,  1, 72, 78, 77,  1, 77, 72,  1, 63, 62, 77, 60, 65,  1, 77, 65, 62,\n",
       "         1, 69, 66, 77, 77, 69, 62,  1, 60, 58, 69, 63,  0, 47, 65, 58, 77,  5,\n",
       "        76,  1, 76, 77, 58, 71, 61, 66, 71, 64,  1, 59, 82,  1, 77, 65, 62,  1,\n",
       "        70, 72, 77, 65, 62, 75,  0, 36, 77,  5, 76,  1, 76, 72,  1, 82, 72, 78,\n",
       "        71, 64,  0, 36, 77,  1, 77, 72, 77, 77, 62, 75, 76,  1, 80, 65, 62, 71,\n",
       "         1, 76, 65, 62,  1, 69, 66, 60, 68, 76,  1, 66, 77,  1, 80, 66, 77, 65,\n",
       "         1, 65, 62, 75,  1, 77, 72, 71, 64, 78, 62,  0, 36,  1, 76, 65, 58, 71,\n",
       "         5, 77,  1, 59, 62,  1, 64, 72, 71, 62,  1, 69, 72, 71, 64,  0, 52, 72,\n",
       "        78,  1, 60, 72, 70, 62,  1, 77, 72, 72,  0])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the same text that we have above... check this using the decoder!\n",
    "data[:335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d3f26d35-ebc5-4503-b06a-a831819930fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484920\n"
     ]
    }
   ],
   "source": [
    "# Training/test split\n",
    "cap = int(len(data)*.9); print(cap)\n",
    "train_data = data[:cap]\n",
    "test_data = data[cap:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1c6e39fa-df84-4731-ba45-e89ee5b5a200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([47, 35, 32,  1, 43, 28, 46, 47, 48])\n",
      "\n",
      "tensor([47]) 35\n",
      "tensor([47, 35]) 32\n",
      "tensor([47, 35, 32]) 1\n",
      "tensor([47, 35, 32,  1]) 43\n",
      "tensor([47, 35, 32,  1, 43]) 28\n",
      "tensor([47, 35, 32,  1, 43, 28]) 46\n",
      "tensor([47, 35, 32,  1, 43, 28, 46]) 47\n",
      "tensor([47, 35, 32,  1, 43, 28, 46, 47]) 48\n"
     ]
    }
   ],
   "source": [
    "# block size/context window\n",
    "block_size = 8\n",
    "print(train_data[: block_size + 1])\n",
    "print()\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1: block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(context, target.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "691ff622-1ff2-4ec5-a7fa-0bfc6f8dae1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      " tensor([[ 0,  0, 42, 63, 63,  1, 63, 75],\n",
      "        [68, 62, 92, 76,  1, 73, 66, 60],\n",
      "        [69, 66, 62, 61,  1, 72, 71,  1],\n",
      "        [69, 69,  1, 80, 62,  9,  1, 76]])\n",
      "--------\n",
      "Outputs: \n",
      " tensor([[ 0, 42, 63, 63,  1, 63, 75, 72],\n",
      "        [62, 92, 76,  1, 73, 66, 60, 77],\n",
      "        [66, 62, 61,  1, 72, 71,  1, 80],\n",
      "        [69,  1, 80, 62,  9,  1, 76, 72]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(314159)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else test_dataset\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i: i + block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1: i+block_size + 1] for i in idx])\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(f'Inputs: \\n {xb}')\n",
    "print('--------')\n",
    "print(f'Outputs: \\n {yb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "6647c2a2-b424-4a95-903d-9b0babcc79ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "-------\n",
      "When the input is tensor([0]) the output is 0\n",
      "When the input is tensor([0, 0]) the output is 42\n",
      "When the input is tensor([ 0,  0, 42]) the output is 63\n",
      "When the input is tensor([ 0,  0, 42, 63]) the output is 63\n",
      "When the input is tensor([ 0,  0, 42, 63, 63]) the output is 1\n",
      "When the input is tensor([ 0,  0, 42, 63, 63,  1]) the output is 63\n",
      "When the input is tensor([ 0,  0, 42, 63, 63,  1, 63]) the output is 75\n",
      "When the input is tensor([ 0,  0, 42, 63, 63,  1, 63, 75]) the output is 72\n",
      "\n",
      "Batch 2\n",
      "-------\n",
      "When the input is tensor([68]) the output is 62\n",
      "When the input is tensor([68, 62]) the output is 92\n",
      "When the input is tensor([68, 62, 92]) the output is 76\n",
      "When the input is tensor([68, 62, 92, 76]) the output is 1\n",
      "When the input is tensor([68, 62, 92, 76,  1]) the output is 73\n",
      "When the input is tensor([68, 62, 92, 76,  1, 73]) the output is 66\n",
      "When the input is tensor([68, 62, 92, 76,  1, 73, 66]) the output is 60\n",
      "When the input is tensor([68, 62, 92, 76,  1, 73, 66, 60]) the output is 77\n",
      "\n",
      "Batch 3\n",
      "-------\n",
      "When the input is tensor([69]) the output is 66\n",
      "When the input is tensor([69, 66]) the output is 62\n",
      "When the input is tensor([69, 66, 62]) the output is 61\n",
      "When the input is tensor([69, 66, 62, 61]) the output is 1\n",
      "When the input is tensor([69, 66, 62, 61,  1]) the output is 72\n",
      "When the input is tensor([69, 66, 62, 61,  1, 72]) the output is 71\n",
      "When the input is tensor([69, 66, 62, 61,  1, 72, 71]) the output is 1\n",
      "When the input is tensor([69, 66, 62, 61,  1, 72, 71,  1]) the output is 80\n",
      "\n",
      "Batch 4\n",
      "-------\n",
      "When the input is tensor([69]) the output is 69\n",
      "When the input is tensor([69, 69]) the output is 1\n",
      "When the input is tensor([69, 69,  1]) the output is 80\n",
      "When the input is tensor([69, 69,  1, 80]) the output is 62\n",
      "When the input is tensor([69, 69,  1, 80, 62]) the output is 9\n",
      "When the input is tensor([69, 69,  1, 80, 62,  9]) the output is 1\n",
      "When the input is tensor([69, 69,  1, 80, 62,  9,  1]) the output is 76\n",
      "When the input is tensor([69, 69,  1, 80, 62,  9,  1, 76]) the output is 72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    print(f'Batch {b+1}')\n",
    "    print('-------')\n",
    "    for t in range(block_size):\n",
    "        context = xb[b][:t+1]\n",
    "        print(f'When the input is {context} the output is {yb[b][t].item()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "3cb92e0e-83e9-4068-a1a9-af474ee64f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 97])\n",
      "4.936264991760254\n",
      "\n",
      "\n",
      ",Re]?{>;L-b[3z[Izh'[UOIl£“-pvw,¬*fc6-j&BKh■JGCz— c”rDfTh5y69|3!pU‘q\n",
      "KxuzXUjdvx\\pN1lU|}Im(8m~Qh£)Zj!5CK‘&&deR?KAv/LEf”O9rs:y5<IIH.db]0K qtif£F<ABM;uWB?—mz/{■(K5b<[¬*P“ltxV’’\n",
      "]YjaFE2//jtX1a(“L]B)k24v[,N.>4B*f9¬;7777jfYI*kh’8!;)Zf^( R6qkUX?—u1>\"VA;77~Z\"n-bJmx\n",
      "Q\n",
      "P6:s\"riHo3k„NLa]O[lPqpFwJpw<66G78^1|DqlQ,BQQp!vg’EAJG\"‘—lx0z\"R[SJVUX3|(\"0 c|zJssA\n",
      "p4e.(0|{q|;¬X<G2■u1?eKAy^&E;X’¬u¬qHLf')RD36Mq\n",
      "s/?~~’’8;)O!}p„W„’0jdHQc\"Y{>7OO6H\"|■I)Z5M/—9VZdYnO.6’*>‘”PvBLfDNtsK!1>AM6G\n",
      "pfp'QZ B“:- h&z\"1q\"kn?‘“aA„U\\0|4O3!qnX\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(134159)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else: \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "        \n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.item())\n",
    "\n",
    "print(decode(model.generate(torch.zeros((1,2), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad917f-ddec-427f-ac68-b534a6a1e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, test_data, get_batch, num_batches=100):\n",
    "    self.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for _ in range(num_batches):\n",
    "        xb, yb = get_batch(test_data)\n",
    "        logits, loss = self(xb, yb)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss/num_batches\n",
    "    self.train()\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "ab03b36b-fb73-4756-a5c8-057f916ee804",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "cd2d9572-e335-4e53-b10f-17b3b2977b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4335415363311768\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "for step in range(10000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "4454229b-2bd8-4196-b447-c426ee4d4728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.555204153060913"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.evaluate(test_data, get_batch, num_batches=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "2bd0a096-4e22-4dd7-a978-ba2a57556373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Towe ren trdishe ckend \n",
      "Ttoupat umid tw watharhey 1 me y w avoowoutonsl I fun t adone HT tee \n",
      "‘Wininond. t t. d acisek-ve hind beruis lagrsoineserso our pes ck. wenyopille arurourr \n",
      "\n",
      "T \n",
      "Tolinopro \n",
      "\n",
      "Choue we tr hew e the’Th t ge bownd s womileat any I camoouthed he \n",
      "\n",
      "\n",
      "'the tharfo s g 'th bathetr alenthed warophes \n",
      "The mery ner \n",
      "\n",
      "Win on 15 otlkerev, ubexe \n",
      "\n",
      "\n",
      "\n",
      "'toed It theld mithe tivey ak P tavere kind e offt o t t at \n",
      "Tincama f GR thathe s \n",
      "\n",
      "AR s to foun hary ake th! I’serod d \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[ the bisas.\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de73b9-4f71-4d34-9e29-9215bdd55213",
   "metadata": {},
   "source": [
    "### Observation... this looks a bit better! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e3f81-a94e-4539-8fd5-5850ac76a5c7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bff014-d3a9-4a1a-a58f-f6df9aae847c",
   "metadata": {},
   "source": [
    "## Part 2. Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "af1143b2-e46b-41ee-b346-c314edd0b497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4,8,32\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "c6dda71b-6796-49df-a04d-2ddeca878307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the naive thing\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "\n",
    "# Next, the matrix version\n",
    "M = torch.tril(torch.ones((T, T)))\n",
    "M = M / M.sum(1, keepdim=True)\n",
    "\n",
    "xbow2 = M @ x\n",
    "\n",
    "# Finally, softmax\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3119043-eb80-4769-bde3-411b27194a35",
   "metadata": {},
   "source": [
    "#### Building up Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "bf27e627-dafc-4128-af7e-aa4def44c1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4,8,32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Attention head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)    # B, T, 16\n",
    "q = query(x)  # B, T, 16\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) --> (B, T, T) \n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "id": "9b6d1b33-2e2e-48fc-bc7b-1f8530b51aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 757,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcdbf66-d14f-4a77-a4a0-213e5abbe78e",
   "metadata": {},
   "source": [
    "NOTE: To control the variance (and avoid one-hot convergence) we divide by the square root of the head_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "26672609-a066-4142-a269-ef09c3568105",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "c1cf4ea3-a761-4c24-834a-45de3cac8ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1201, grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "96840284-92a1-4413-885d-1cad8d22279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "1db793cb-61c7-46e6-a4a6-10f7d0acdcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0632) tensor(0.9891) tensor(0.9755)\n"
     ]
    }
   ],
   "source": [
    "print(k.var(), q.var(), wei.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2626512-edc4-45ba-a9bc-c02a27cbb3ba",
   "metadata": {},
   "source": [
    "### Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "0898d320-9a47-4b40-a3a1-5c70725f30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae2177da-0677-4822-876a-42b0025a83ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yoda.txt\n",
      "Vocab size: 60\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 16\n",
    "max_iters = 5000\n",
    "eval_iter = 500\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 500\n",
    "n_embed = 32\n",
    "dropout = 0.3\n",
    "\n",
    "\n",
    "torch.manual_seed(314159)\n",
    "\n",
    "files = ['RobertFrost_poems.txt', 'Dwight.txt', 'yoda.txt', 'quote_bank.txt', 'shakespeare.txt']\n",
    "file = files[2]\n",
    "print(file)\n",
    "\n",
    "dat = open(file).read()\n",
    "chars = sorted(list(set(dat)))\n",
    "\n",
    "\n",
    "vocab_size = len(chars); print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "stoi = {char: i for i, char in enumerate(chars)}\n",
    "itos = {i: char for i, char in enumerate(chars)}\n",
    "encode = lambda str: [stoi[char] for char in str]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "data = torch.tensor(encode(dat))\n",
    "cap = int(len(data)*.9)\n",
    "train_data = data[:cap]\n",
    "test_data = data[cap:]\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else test_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embed//n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ff = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        \n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embed, n_heads=4),\n",
    "            Block(n_embed, n_heads=4),\n",
    "            Block(n_embed, n_heads=4)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else: \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)           \n",
    "            logits = logits[:, -1, :]          \n",
    "            probs = F.softmax(logits, dim=1)         \n",
    "            idx_next = torch.multinomial(probs, num_samples=1)       \n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "452ff8d2-60ec-4b72-ba4e-fd83d70eea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c9850e5f-c76e-415a-b5f4-7c1a7e7a775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, Train loss: 1.7504,  Test loss: 2.2475\n",
      "step: 500, Train loss: 1.6759,  Test loss: 2.2233\n",
      "step: 1000, Train loss: 1.6110,  Test loss: 2.2175\n",
      "step: 1500, Train loss: 1.5504,  Test loss: 2.2154\n",
      "step: 2000, Train loss: 1.4996,  Test loss: 2.2003\n",
      "step: 2500, Train loss: 1.4485,  Test loss: 2.1913\n",
      "step: 3000, Train loss: 1.4194,  Test loss: 2.1770\n",
      "step: 3500, Train loss: 1.3881,  Test loss: 2.1869\n",
      "step: 4000, Train loss: 1.3585,  Test loss: 2.1910\n",
      "step: 4500, Train loss: 1.3364,  Test loss: 2.1776\n",
      "\n",
      "Youd must hern your of that boy you for ate wwaoone of thicy learnewploweren as tarf you ghas end nothern and? On to losseens best. Us the is frond a not in.\n",
      "You, for your tairoy of this ming Skywaing the butu..:-hen thas.\n",
      "Master dious, teng froutugh of this. Emporture have by.\n",
      "Ity.\n",
      "I you what go.weerentight hay is, cul al do, is not a Jedi to cail.\n",
      "Yossee betwaithe he Forned. Obwan fear lik wet Suturn not gWraiatiny toy sumplainet am, this one dares.\n",
      "No. Beng If the camesto net. Lus a datas is \n"
     ]
    }
   ],
   "source": [
    "for step in range(max_iters):\n",
    "\n",
    "    if step % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {step}, Train loss: {losses['train']:.4f},  Test loss: {losses['test']:.4f}\")\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fdff79b3-2bb3-41b9-b4d0-0b54a5e00d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iund preriesth it path is thorce rearnst ist. At.\n",
      "Das terywith at leave th to train he comp oner strond that yearns ingse the Fordilys undiouce.\n",
      "The if lest you dest. Untrassivo they. Mathere if ap.\n",
      "Notade, of gre, are aif ays it, your spentror ong to filly you the dark stayset bewars a nkeed, patich tome ter, of thace morce trasce the the lay.\n",
      "alway the Fory Clou whit eat the Sumep to of thelme, to that wille side.\n",
      "ain. Soming thim, mis nacrentoucheng gone must.\n",
      "Clit. Bust wo the dark side in i\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbe2eddf-fedd-4c52-9b73-458cf96c9edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block(\n",
       "  (sa): MultiHeadAttention(\n",
       "    (heads): ModuleList(\n",
       "      (0-3): 4 x Head(\n",
       "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "  )\n",
       "  (ff): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06a65a-3baf-470d-9085-201294fbc044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
